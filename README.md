# Yalom - Huberman Lab AI Assistant

AI-powered search and Q&A system for Huberman Lab podcast transcripts using RAG (Retrieval-Augmented Generation).

## âœ¨ Features

- ğŸ¤– **RAG Pipeline** - Intelligent Q&A with context retrieval
- ğŸ” **Query Rewriting** - Enhanced search coverage
- ğŸ’¬ **Streamlit Web UI** - Beautiful chat interface
- âš¡ **Groq API** - Fast LLM inference (free tier: 14,400 req/day)
- ğŸ¯ **HuggingFace Embeddings** - Local, free sentence-transformers
- ğŸ—„ï¸ **Dual Vector Stores** - Qdrant (local) + Pinecone (AWS)
- â˜ï¸ **AWS Serverless** - Lambda deployment (~$0.02/month)
- ğŸ¥ **Multiple Sources** - YouTube, Kaggle, GitHub transcripts
- ğŸ“ **Timestamped** - Precise source attribution
- ğŸ³ **Docker Ready** - One-command local setup

## ğŸš€ Quick Start

```bash
# 1. Install dependencies
make install

# 2. Set up environment
cp .env.example .env
# Edit .env and add: GROQ_YALOM_API_KEY=your-key

# 3. Start dev environment
make dev
# Opens Streamlit at http://localhost:8501
```

## ğŸ“ Project Structure

```
yalom/
â”œâ”€â”€ src/                    # Core library code
â”‚   â”œâ”€â”€ database/          # Database models & connections
â”‚   â”œâ”€â”€ ingestion/         # Data fetching & scraping
â”‚   â”œâ”€â”€ rag/              # RAG pipeline & prompts
â”‚   â””â”€â”€ vectorization/    # Embeddings & vector store
â”œâ”€â”€ app/                   # Streamlit web UI
â”œâ”€â”€ scripts/              # CLI tools & utilities
â”œâ”€â”€ tests/                # Unit & integration tests
â”œâ”€â”€ aws/                  # AWS deployment (IaC)
â”‚   â”œâ”€â”€ lambda_ingestion/ # Ingestion Lambda
â”‚   â”œâ”€â”€ lambda_query/     # Query Lambda
â”‚   â”œâ”€â”€ terraform/        # Terraform configs
â”‚   â””â”€â”€ deploy.sh        # Automated deployment
â”œâ”€â”€ docs/                 # Documentation
â””â”€â”€ Makefile            # Common development tasks
```

## ğŸ› ï¸ Development Commands

```bash
make help              # Show all commands
make install          # Install dependencies
make test             # Run tests
make lint             # Run linting
make format           # Format code

make docker-up        # Start Qdrant
make docker-down      # Stop Qdrant
make run-streamlit    # Run Streamlit UI
make populate-db      # Populate vector database

make aws-deploy       # Deploy to AWS
```

## ğŸ“Š Data Sources

This project supports multiple transcript sources:

### 1. Kaggle Dataset (197 episodes)
- **Source**: https://www.kaggle.com/datasets/tkrsh09/huberman-lab-podcast-transcripts
- **Format**: Timestamped transcripts
- **Load**: `poetry run python utils/download_kaggle.py`

### 2. GitHub Repository (134 episodes)
- **Source**: https://github.com/prakhar625/huberman-podcasts-transcripts
- **Format**: Markdown transcripts
- **Load**: `poetry run python scripts/cli.py github-load`

### 3. YouTube Direct Scraping
- **Source**: YouTube API via youtube-transcript-api
- **Format**: Auto-generated captions
- **Load**: `poetry run python scripts/cli.py scrape-channel URL`
- âš ï¸ **Note**: Rate limited, use Kaggle/GitHub instead

## â˜ï¸ AWS Deployment

Deploy serverless infrastructure with Lambda + Pinecone:

```bash
# 1. Configure
cd aws/terraform
cp terraform.tfvars.example terraform.tfvars
# Edit with your API keys

# 2. Deploy
make aws-deploy

# 3. Test
curl -X POST https://YOUR_API.execute-api.us-east-1.amazonaws.com/query \
  -H "Content-Type: application/json" \
  -d '{"query":"What is neuroplasticity?"}'
```

**Monthly Cost:** ~$0.02

See [aws/README.md](aws/README.md) for complete guide.

## ğŸ“š Architecture

### Local Development
```
Streamlit UI â†’ RAG Pipeline â†’ Qdrant (Docker) â†’ Groq API
                â†“
        HuggingFace Embeddings (all-MiniLM-L6-v2)
```

### AWS Production
```
API Gateway â†’ Lambda â†’ Pinecone â†’ Response
               â†“
       Groq API + HuggingFace Embeddings
               â†“
       S3 (transcript backup)
```

## ğŸ“– Documentation

- [AWS Deployment Guide](docs/AWS_DEPLOYMENT_COMPLETE.md)
- [Groq Integration](docs/GROQ_INTEGRATION.md)
- [Query Rewriting](docs/QUERY_REWRITING.md)
- [Configuration Options](docs/CONFIGURATION.md)
- [Kaggle Dataset Guide](docs/KAGGLE_DATASET_GUIDE.md)
- [GitHub Data Source](docs/GITHUB_FREE_SOLUTION.md)

## ğŸ› Troubleshooting

**Qdrant not running:**
```bash
make docker-up
docker ps  # Verify qdrant container
```

**Groq API key error:**
```bash
export GROQ_YALOM_API_KEY="your-key-here"
make check-env
```

**NumPy compatibility:**
```bash
poetry add "numpy<2.0.0"
poetry install
```

## ğŸ¤ Contributing

1. Fork the repository
2. Create feature branch: `git checkout -b feature/amazing`
3. Make changes and test: `make test lint`
4. Format code: `make format`
5. Commit: `git commit -m 'Add amazing feature'`
6. Push: `git push origin feature/amazing`
7. Open pull request

## ğŸ“ License

MIT License

## ğŸ™ Credits

- **Data**: [Huberman Lab Podcast](https://hubermanlab.com/)
- **LLM**: [Groq](https://groq.com/) (llama-3.3-70b-versatile)
- **Embeddings**: [HuggingFace](https://huggingface.co/) (all-MiniLM-L6-v2)
- **Vector DB**: [Qdrant](https://qdrant.tech/) / [Pinecone](https://www.pinecone.io/)

---

Made with â¤ï¸ for the Huberman Lab community

### Prerequisites

- Python 3.8.1+
- PostgreSQL (or Docker)
- Poetry

### Installation

1. Clone the repository
2. Install dependencies:
   ```bash
   poetry install
   ```

3. Start PostgreSQL:
   ```bash
   docker-compose up -d
   ```

4. Initialize the database:
   ```bash
   poetry run python cli.py init-db
   ```

## Usage

### Command Line Interface

**Scrape an entire channel:**
```bash
poetry run python cli.py scrape-channel "https://www.youtube.com/@channelname"
```

**Scrape with custom delay:**
```bash
poetry run python cli.py scrape-channel "https://www.youtube.com/@channelname" --delay 3
```

**Scrape a single video:**
```bash
poetry run python cli.py scrape-video VIDEO_ID
```

### Programmatic Usage

```python
from src import init_db, get_db_session, ChannelScraper

# Initialize database
init_db()

# Create a session and scraper
session = get_db_session()
scraper = ChannelScraper(session, delay=2)

# Scrape a channel
stats = scraper.scrape_channel("https://www.youtube.com/@channelname")
print(f"Successfully scraped {stats['success']} videos")

session.close()
```

## Project Structure

```
yalom/
â”œâ”€â”€ src/                    # Main source code
â”‚   â”œâ”€â”€ config.py           # Configuration settings
â”‚   â”œâ”€â”€ database/           # Database models and connections
â”‚   â”œâ”€â”€ ingestion/          # Data ingestion logic
â”‚   â”œâ”€â”€ vectorization/      # Embedding and vector storage
â”‚   â””â”€â”€ rag/                # RAG pipeline and prompts
â”œâ”€â”€ app/                    # Applications
â”‚   â””â”€â”€ streamlit_app.py    # Web UI
â”œâ”€â”€ scripts/                # Utility scripts
â”‚   â”œâ”€â”€ chat_cli.py         # Terminal chat interface
â”‚   â”œâ”€â”€ populate_vector_db.py  # Vectorize transcripts
â”‚   â”œâ”€â”€ clean_timestamps.py # Clean transcript timestamps
â”‚   â””â”€â”€ view_prompts.py     # View prompt versions
â”œâ”€â”€ tests/                  # Test files
â”‚   â”œâ”€â”€ test_rag.py
â”‚   â”œâ”€â”€ test_multilingual.py
â”‚   â”œâ”€â”€ test_similarity_filtering.py
â”‚   â””â”€â”€ ...
â”œâ”€â”€ migrations/             # Database migrations
â”œâ”€â”€ docs/                   # Documentation
â”‚   â”œâ”€â”€ CONFIGURATION.md
â”‚   â”œâ”€â”€ QUERY_REWRITING.md
â”‚   â”œâ”€â”€ SIMILARITY_FILTERING.md
â”‚   â””â”€â”€ ...
â”œâ”€â”€ cli.py                  # Command-line interface
â”œâ”€â”€ docker-compose.yml      # PostgreSQL container config
â”œâ”€â”€ pyproject.toml          # Poetry dependencies
â””â”€â”€ README.md
```

## Configuration

Environment variables (optional, see `.env.example`):

- `DATABASE_URL`: PostgreSQL connection string
- `LOG_LEVEL`: Logging level (DEBUG, INFO, WARNING, ERROR)

## Best Practices Implemented

- âœ… Separation of concerns (models, database, business logic)
- âœ… Type hints throughout
- âœ… Comprehensive logging
- âœ… Context managers for resource management
- âœ… Error handling and retries
- âœ… Configuration management
- âœ… Documentation and docstrings
- âœ… Production-ready database (PostgreSQL)
- âœ… Rate limiting to respect YouTube's limits
